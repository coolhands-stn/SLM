{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B7lOfWLKXH3",
        "outputId": "f1f478ba-cdb1-4ea1-8d60-ad2b42722ac4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/rugare.txt\",\"r\")\n",
        "text = file.read()"
      ],
      "metadata": {
        "id": "nN-CALwFMU8P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mzAfsjVpN1F_",
        "outputId": "7097772c-a6d6-4991-9f73-d20d0eb8026d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zita rekuberekwa ndinonzi Rugare, ndiri mwana wechi6 mumhuri medu takaberekwa tiri 8 vakomana vatatu nevasikana 5. Kuvasikana ndiri wechi4. Ndine makore\\t18 ekuberekwa. Chikoro ndakagumira hangu paOlevel yandakapasawo zvekubatanidzira asi handishori hangu Mwari.\\n\\n\\nMushure mekupedza 0 level yangu ndakambogara pamba pevabereki vangu ndichitoshaya kuti ndotangira papi.\\nMumusoro mangu mabasa andaifunga kuti akanaka ndiyo nursing neteaching saka ndaingotiwo ndichazoedzawo hangu\\n \\nnursing sezvo teaching ndaitoityawo.\\n\\nKuchurch ndaienda chaizvo ende Mwari ndaimuda zvakanyanya sezvo ndakakurira mumhuri yainamatwa.\\n\\nNdiripo pamba pevabereki vangu kudaro ndayiita mabasa ese anoitwa kumusha.\\nTaigara nemasisters angu maviri nekahanzvadzi kaiva kagotwe nababa naamai. Vamwe vakanga vakura vava nemhuri dzavo.\\n\\nRimwe zuva tichibva hedu kuchurch iri Sunday ndakawana pamba pedu pane cousin sister yedu achiti anga awana munhu ayida musikana webasa\\n \\nwemumba. Nekuda kwekuti ndakanga ndakusvotwa nekugara pamba ndakatoona kuti zvaiva nani ndiende zvangu kunoshanda. Mufaro wangu wayiita kufashukira kani kuti ndakunogarawo mutown uye ndakuzobatawo mari inoti ini. Handina kutora nguva kabag kangu kange katorongedzwa.\\n\\n\\nHandina kusiyawo mapepa angu ekuchikoro semunhu ayizodawo kuapplier kunursing.\\nTichisvika paimba pandaizoshanda ndipo pandakaona kuti ndakutanga hupenyu.\\nNdakatanga kupihwa mitemo inegumi. Pamba apa painzi paMlauzi. VaMlauzi vaisada kuti vana vavo vatauriswe\\n \\nneshona.\\n\\n\"Hey Rugare. It\\'s either you speak with my children in English or Ndebele, no Shona here\"\\n\\n\\nRakavomba rume. Ini semuenzi ndakanga ndangozvipeta hangu ndiri mukakona.\\nNdakatorara ndatoudzwa kuti ndaizomuka na4:30 nekuti vanhu vaizofanira kumuka ndapedza mabasa ese epamba.\\nNdaingogutsurira musoro asi kutaura chokwadi moyo wakanga wava kwamai vangu. Amai vacho vairatidza zvavo kuva munhu akanaka asi chakandirwadza ndechekuti vaisashandira muZvishavane mataigara vaienda kuMasvingo every Monday vachidzoka Friday manheru.\\n \\nPandakanzwa izvi moyo wangu wakatanga kupisa ndichifunga Kuti ndaizogarisana sei nerume rinenge vaMlauzi.\\n\\nZuva iri husiku hwakandiitira hupfupi chose.\\nNdakatarisa pawatch yaiva kumadziro mumba mandairara ndikaona dzatova 4 am ndikatoona kuti hapachina chekurarira apa. Ndakabuda panze ndikatsvaira chivanze changu bhoo bhoo. Kushanda ndaiva zvangu munhu aishanda kungoti semunhu anga asina kumbonyatsogara mutown zvimwe zvaindivharawo asi ndaive nani hangu.\\n\\n\\nNdakazoona kwava kumapast 6 mai\\n \\nMlauzi vakundioneka nekundiraira kekupedzesera voenda havo kuMasvingo kwavaishandira.\\tZuva rakasara robuda amira mira mwanasikana wekwaJekwa zvese zvava panzvimbo.\\nNdakazogadzira break fast tikadyira tose patable. Ndaida zvangu kugara kuKitchen asi vaMlauzi vakarambisisa. Hana yangu yayiita kurova neziso randaichekwa naro nababa ava.\\n\\n\\nNdanga ndisina kutaura.\\tndiri\\nmusikana akaumbwawo zvake zvirinani uye ndine kakukangavira. Taigara zvedu kumusha asi baba vangu vanga vari pensioner saka taiwanawo zvirinani.\\n \\n\\nTakapedza hedu breakfast zvakanaka semunhu ayiva mutsva pabasa ndaingomira mirawo kuda kufadza murungu. Ndaiti ndikangocheuka ndoona iyaaa ndakatariswa. Ndakazonzwa shasha yakutaura pafon. Vati torei time pafon vachibva vandipa sign yekuti ndaidiwa pafon. Ndakazvininipisa ndichigara pasi pedo nefon vakanditambidza apa hana yaiita kubika manhanga. Kamufaro kakazouya ndanzwa kuti ndimai Mlauzi vaitaura. Vaingoda zvavo kuziva kuti ndaiva sei nevana vavo uye basa raisarema here.\\n \\nNdichipedza kutaura pafon ndakavadzorera hangu ndikanzwa ndabatwa magadziko. Ndakanzwa hasha dzisingaiti. Chokwadi baba ava vaindishurira. Ndakacheuka kwakuona rume ririkuita kuzhinya apa kuzonditsonya manje.\\n\\n\\nNdakabva ndatozviudza kuti pano handaizoparara.\\nNdakasiyana nazvo hangu ndikati regai ndipinde mubathroom ndigeze semunhu akanga aneta. Ndichingokiya door ndakanzwa feeling yekuti padoor paiva nemunhu. Ndakazonzwa kasoft knock hanzi nekazeve zeve.....\\n \\n\"Rue vhura ndikudawo kugeza vhura tigeze tese.\"\\n\\n\\nPamba apa ndaichengeta vana vatatu muhombe aiva musikana ayiva mugrade six koita vakomana vadiki 2 umwe ayiva mugrade 4 umwe grade 1.\\n\\n\\n\\nNdakamboramba ndakamira ndichifunga kuti pamwe ndairota. Asi chii shuwa baba ava vaindifungira chii? Ndakabva ndapfuurira mberi nekugeza zvangu asi moyo wangu wanga wazara kutya nekuti\\n \\nndakanga ndatoona kuti pano ndirikugara nebhinya. Ndakazongonzwa mafootsteps munhu akubva padoor.\\n\\n\\n\\nNdichipedza kugeza ndakakasira kuchinja hembe semunhu akanga akutya kubatwa chibharo nekuti ndakanga ndatoona kuti ndipo paiva nepfungwa dzababa ava. Ndakatanga kudzorera hembe dzangu dzose mubag dzandaiva ndaisa muwardrobe.\\n\\n\\nKo ndairegereyi hangu ini ndakanga ndafunga kutiza? Ndaizoparara sei pamba apa nebhinya rinenga Mlauzi. Chokwadi munhu ayiva nemukadzi wake musvinu\\n \\nkuchivawo pwere inenge inini. Apa chero kugeza kwacho ndakanga ndisati ndakumbogona.\\n\\n\\nAva vaMlauzi ndakazonzwa kuti vaiva nemakombi avo saka chavo kungogashira mari inenge yauya nevanochovha nekuona kuti makombi avo pane zvinoda kugadziriswa here. Ivo vakanga vachimbova truck driver. Kubva vaita accident saka vakanga vasisakwanise kushanda nekuda kweaccident. Saka chavo kwaiva kugara pamba vachishusha zvavo.\\n\\n\\n\\n\"Rue! Come here I want to send you\\n \\nsomewhere!\"\\n\\nNdakadududza ndichienda kwavaiva ko hanti taitofanira kutaura chirungu apa ini chaindivhara zvangu kkkkkk. Vakazondiudza kuti ndaifanira kunotenga brake fluid kudii weee ini zvangu ndakatoona kuti kuvaudza kuti brake fluid yacho handiizive kwaiva kupedza nguva nekuti ndiyo chance yangu yandanga ndawana yekubva paimba apa. Ndakatambidzwa mari yacho yandaizotenga brake fluid yacho apa ini ndaisamboziva zvangu kuti chimbori chii ichocho. Shasha ichipinda mubedroom mayo ndakamhanyawo muspare mangu kwakutora kabag kangu ndiye dhugu kubuda zvekwandakanga ndarairwa kwacho kwaitengwa brake fluid\\n \\nndaiva ndisineyi nako.\\n\\n\\nNdakasangana nekamwana kavo kamusikana kainzi Senzeni kari panze pegate kachitamba. Ndikakapa Mari iya ndikati hanzi nadad vako tenga brake fluid. Ndakaitira kuti ndisazonzi ndaba mari yevanhu ndikatiza. Ko yaimbove ngani mari yacho asi ndakati handidi zvangu kuzvipa munyama.\\n\\n\\nNdakamhanya veduwe ndakananga kubasa kwasisi vangu vaya vekuuya neni. Vakashamiswa kundiona ndichisvika nekabag. Ndakavarondedzera zvese zvainge zvaitika ndikafara kuti havana kundipa mhosva asi vakatoti wagona\\n \\nukatiza. Vakazofonera murume wavo vachimuonesawo zvanga zvaitika.\\nBamukuru vakazouya vakaratidza kufara chaizvo kuti takugara namainini.\\n\\n\\nNdakamboedza kuramba ndichiti ndaida kudzokera kumusha asi vose vakatsika madziro. Hanzi kumusha hakuite hatizowane vana bamunini vanoonekera kudii ko zvaimbova mumusoro mangu here zvekuita mukomana.\\n\\n\\nSisi vangu nemurume wavo vakanga vava ne 1 year and some months varoorana zvavo asi vanga vasati vave nemwana.\\nChaingondifadza zvairatidza kuti vaiwirirana chose.\\n \\n\\nZuva rakanonoka kudoka ndichida zvangu kunozorora kumba kwasisi vangu ndichifunga nhamo dzangu dzandaive ndasangana nadzo. Hameno wo kuti Senzeni akazotenga here brake fluid yacho handina zvangu kuzomboita basa nazvo asi ndaifara kuti ndakanga ndabuda mumuromo mamupere.\\nChokwadi dayi ndanga ndakazvipusira zibaba riya raigona kuzonditorera humhandara hwangu. Ndakaita kamunamato kekutenda Mwari kuti chokwadi mandigonera.\\n\\n\\n\\nTakazoenda hedu kumba ndikashamiswa\\n \\nkuona kt sisi nababamukuru vaigara muone room. Saka taizogara sei. Idzi dzaiva pfungwa dzaindinetsa asi ndakangoti regayi ndisiire varidzi vemba. Room yacho yaiva zvayo hombe vachitova netuproperty twezera ravo but inini zvangu I was not comfortable.\\nNdaitogaya kuti pano handigari chokwadi kutadzisa vanhu kufara zvavo ini ndichiti ndinovavarireyi kkkkk. Pakazoitwa plan yekuganhura room necurtain asi kwandiri ndaingoona hangu kuti taiva mu one room.\\n\\nChizvarirwo changu handina kumboziva kuti zvinoita kusheya room nevanhu vakaroorana. Ndakawaridzirwa hangu magumbeze\\tpasi petable. Zvimwe ndezvimwe sisi vangu magumbeze vaiva nawo. Ndakazviyemura ndikati\\n \\nndichadawo kuzotengawo akawanda kana ndaita mari dzangu. Isuka kumusha taitorara zvedu tichidhonzerana maviri iwayo tiri vatatu. Asi uku ndakaisirwa maviri mahombe pasi ekufuga ndikapihwa 2 in 1 nerimwe ravakati rainzi comforter.\\nKo ndaimboziva kt magumbeze ane mazita.\\n\\n\\n\\nHope hadzina kunonokawo kundibata semunhu akanga akarara pamutepfe tepfe.\\tBamukuru\\nvangu vainzi Shumba\\tuye vaiva munhu anofara chaizvo. Ndakangonzwa Kuti vaishanda kumabhazi asi handizivi Kuti vayiita basa rei. Sisi vangu vaishanda mushop kureva Kuti the following day\\n \\nkana Kuti mazuva mazhinji ndaizoswera ndega hangu pamba pasisi vangu.\\n\\n\\nSaka zuva raitevera ndakazomuka hangu shasha dzatogeza dzichitoyemerana hadzo dzichigadzirira kuenda kubasa.\\nTakabvunza na kumuka ndikatanga hangu kupeta macomforter nema2 in one angu.\\nKo hanti akange atove angu. Papi pacho pavaizonditorera kkkkk\\n\\n\\n\"Mainini Ruga munombotaurawo?\"\\n\\nVakadaro bamukuru vangu vachisekerera vachibudisa dimple ravo.\\n \\n\"Ruga ndicho chii nhaiwe Shumba? Hindava uchida kushatisa zita remwana wamai vangu? Better kt Rugare kana usingadi kuti Rue.\"\\n\\n\\nVakadavira sisi vangu vachitoratidza Kuti vairevesa. Ini ndakangoti hameno ikoko pedzeranai nechemumoyo\\n\\n\\nMainini zvenyu ini kuti Rue kunongondibhohwa saka ndichakutii Ruru.\\n\\n\\nZvakanakayi bamukuru.\\n \\nNdakadavirira zvangu pasi ndichitoshaya kuti vanhu vanganetsane nekuda kwezita rangu here shuwa.\\n\\n\\nHama dzakazoenda kubasa ndikasiiwa ndaratidzwa chikafu ndaizongobika zvandaida. Mumba maivewo nekafridge kezera ravo. Kwandiri zvakanga zvakarongeka kusara kwekurara kwatayiita mu one room.\\n\\n\\nMazuva akafamba ndiripo hangu pamba pasisi vangu. Taigara zvedu zvakanaka asi shasha dzaiti dzikatanga kuita gakava ndaizopedzisira ndasvotwa ini. Kunamata nekuenda kuchurch handina kurega hangu. Ndakanga ndakuzikamwawo\\n \\npaAssembly yandaipindira kuMandava nekuti ndaimbopota ndichishumirawo muchurch. Hembe sisi vangu vakandipawo dzimwe dzaionekera dzekuendesa kuchurch hanzi madende aya ekwaMapanzure angatishaisa vana bamunini. Asi sisi vangu so vaimbondipedza power madire avayiita vana bamunini.\\n\\n\\nBamukuru takanga takunzwanana hedu. Vaitoti kana vachienda kunotenga doro vaiti ndoperekedzwa namainini. Ndanga ndisina kukuudzayi, bamukuru vangu nasisi vaipinda Anglican asi vaipota havo vachibira kunwa doro. Kana zvakakwidza sisi vaimbonwira nwirawo. Bamukuru vakazotangawo kungondipa kamwe\\n \\nkaziso kandaisanzwisisa asi ndakangotiwo hameno. Pamwe pacho vaiti vachibva kubasa vaida kundikwazisa neruoko vobva vandikwenya.\\nNdakamboda kubvunza sisi kuti zvaireveyi asi ndakazongotyawo.\\n\\n\\n\\nKungogezewo mvura yesoda kwakanaka asikana. Ndakanga ndaita kubuda Rugare chaiye. Kukangavira kuya kwakanga kwava kutsvuka. Apa nekuzodyawo tumazai zai timahips twakanga twabudawo. Vana baba kumusha vakangoudzwa kuti Ruru tisu tava naye tichamutsvagira rimwe basa. Ukuwo bamukuru vaiva nenhema dzinorura vakatoudza vana baba vangu kt vaiva\\n \\nnehama yaigona kuzonditsvagira basa kunursing. Mai vangu vakapururudza vakati zvedu zvaita tinenge tava naMukoti mumba.\\n\\n\\nNdakambosangana namainini vangu mutown vakandibvunza Kuti taigara sei mu one room. Ndakangovati zvanga zvisina kana problem asi vakaratidza Kuti havana kufara nazvo.\\n\\n\\nRimwe zuva iri Sunday ndaitofamba hangu nemamwe mabrother tichienda kumba ivo vaizodarikira havo vachienda kwavo. Ko mabrother akanga akupenyerwawoka kkkkkkk ko hanti ndakanga ndavawo sister Ruru\\n \\nvemupraise team. Ndakanga ndatova ne2 and half months ndichigara nasisi vangu muone room imomo.\\n\\n\\nTaifamba zvedu zvishoma nezvishoma nyaya dzichitswa.\\nKana vari sabrother Doubt vaiva nenyambo dzekuti waiseka kusvika misodzi\\tyabuda. Tapedo nekumba bamukuru vangu vakangopfuura vasina kana wavamhoresa apa vachifaambisa.\\nMaswera sei bamukuru?\\'\\n\\nNdakadaro ndichidududza kutevera kumashure kwavo.\\n \\nBible rangu raiva rakabatwa nabrother Wisdom saka vakabva vandideedza.\\n\\n\"Rue! Bible rako!\"\\n\\nAva vaisada zvekuti sister hanzi unozoita kunge wakapikira kuRoma .Ndakabva hangu ndadzoka kumashure tikazoonekana zvakanaka naBro Wisdom naBro Doubt. Ndichipinda mumba ndakagashirwa neshura. VaShumba vanga vakaita kutsvukisa ziso. Handsome yose kutama munhu akawonyanisa face kuita sendove.\\n\\nInguvai yamurikusvika pano? Church yapera nguvai? Ndivanaani vamanga muchinanaidzana navo?\\n \\nMakuda kuita chihure pano?\\n\\n\\nGegeede kuseka zvangu nhamo serugare handina kana kukwanisa kupindura mubvunzo umwe zvawo. Sisi vangu vaiva vakagara pastool vakabata shaya.\\nNdakatoshaya Kuti chatsamwisa chii. Sisi vangu ndivo vakazodaira havo nokuti ini pfungwa dzanga dzisisiri panzvimbo.\\n\\n\\nMai Shumba:Hamuvazivi here vakomana vavanopinda navo Inga ndambokuudzayi wani.\\n\\n\\nVakadaro mai Shumba. Upenyu hwangu ndakanga ndisina kumboita misikanzwa\\n \\nsaka izwi rekuti makuda kuita chihure rakauya rikaita kugara pamoyo. Ndainzwa ndakaitwa kugarwa nechigodo pahuro.\\nMisodzi ndiyo yakazongoti mokoto mokoto nematama. Ndakaona Kuti pakanga pasina chaida kuexplainiwa apa asi Ivo ndivo vaitofanira kunditsanangurira chakanga chavatsamwisa.\\nKwakazodoka shasha dzoyemerana asi ini ndakanga ndakutogaya zvakawanda. Ndakambofunga kudzokera kumusha asi ndaiti ndikagaya kunobata gejo hana yangu yairova. Musandishora veduwe ukaravira town wakurira kumusha unozoona zvisisaite kudzokera kunogara ko zve.\\n \\nManheru sisi vangu ndivo vakabika ini nekudya handina. Bamukuru vakamboedza kuita tunyambo vachiti ndichaseka asi ndakati handisi kuzodzora tsvimbo. Nguva yekurara yakasvika ndikagadzira pandairara asi hope hadzina kuuya. Nyaya yekudzokera kumusha yaiuya ndichiipikisa zvangu.\\nNdakazobatikana ndonzwa vanhu vofadzana zvavo Kuda vaifunga kuti ndararara. Hope dzakazongondibawo pave paya. Ndakarota ndichida kuwira mudziva raiva nemvura yakaita black apa mune zigarwe ranga takaita kushamira muromo. Mainini vangu vaya ndivo vakauya vachiita kunge vaibhururuka vakandibvuta vakandikandira Paiva neloan yanga yakasvibira. Ndakapepuka ndikaona kwatochena hama dzaenda\\n \\nhadzo kunogeza.\\nNdakazviudza Kuti nhasi handitsamwi ndikati ndakuda kubva vaizondirambidza. Vachitopinda ndakasekerera ndichivabvunza kumuka.\\nTakazoparadzana kuenda wangova mufaro bedzi bedzi.\\n\\n\\nPasina kana 2 hours bamukuru vanga vadzoka zvavasingamboiti. Ndaitova zvangu panze ndichiita nyaya nevavakidzani. Vakangondipa sign yekuti handeyi mumba. Vakagara havo pastool ini ndikaramba ndakamira ndakavatarisa. Vakaisa ruoko muhomwe vakaburitsa rino zichocolate\\tndofunga ndiyo size hombe yacho.\\nKO garaika pasi timbotaura.\\n\\n\\nVakadaro vachisekerera vachiburitsa dimple ravo riya. Vanga vakaumbwa havo bamukuru vangu. Sisi vangu vaitokurirwa kunaka pafeya. Ndakafinyama Kuti vasafunga Kuti ndavayemura.\\nNdakadhonza stool ndikagara nechekure. Vakabva vakanda Chocolate Iya pamakumbo pangu ndakayiignowa sendisingaidi chero mumoyo ndaifara kuti ndomboiravirawo. Ko vanaRue taingodziona nemeso mumashop umu.\\n\\n\\nBamukuru: Nyaya iri pano mainini muchiona ndakadzoka kudayi ndati tinzwisisane sevanhu vakuru. Ini nasisi\\n \\nvenyu tava nealmost 2 years taroorana asi hapana mwana. Manje imi zvamuri pano kudayi munogona kutibatsira. Sisi venyu handina hangu kutaura navo asi ndichavaudza. Kana vana tete venyu ndikavaudza kuti ndafunga kuti mainini vabatsirane nasisi vavo ndoona vanofara pakuti ndibude kunze .......\\n\\n\\nMashoko aya ndakanga ndisina kuatarisira kuna babamukuru vangu. Ndakaita kunonzi kurohwa nesurprise chaiyo. Ndakashama muromo uku ichiita kubvunda nehasha. Chokwadi baba ava vaindishurira manenji chaiwo. Inini chaiyi munhu akanga asati asvika 19 ndakanga ndakunzi ndiende pachipare. Apa chero mukomana ndakanga ndisati ndava naye.\\n \\nNdakangogona Kuti bamukuru budai muno ndisati ndaridza mhere ndichivatema nezichocolate ravo.\\nHameno akavanyepera kuti vasikana vekumusha ukangomupa chocolate wamuhwinha.\\n\\n\\nNdakasara ndikahwihwidza ndega mumba imomo.\\tNdakachema kusvika ndabatwa nehope. Ndakanga ndichiti ndicharongedza hangu kabag kangu ndozooneka sisi vangu kana vabva kubasa. Ndaisada hangu kuvarwadzisa ndichivaudza zvanga zvaitwa nemurume wavo. Asi vakasvika pakudzoka ndakangozvinetera zvekurongedza.\\n \\nVachiuya\\tshasha dzaingoyemerana semazuva ose uku bamukuru vaingovhara vhara vasingadi kana kutarisana neni. Takarara hedu chifume vakamukira semazuva ese. Ndichiri kucleaner mumba ndichitokorobha hangu ndakanzwa door ravhugwa munhu achitopinda nekulocker door nekuisa makey muhomwe. Ndakangosimuka zvekujamba zviya ndichimira necheuko.\\nBabamukuru vanga vakaita kutsvukisa ziso. Ini ndakazviudza Kuti pano ndinotofanira kumira kana kuchifiwa ngakufiwe\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle"
      ],
      "metadata": {
        "id": "2CBTkJ5LMcbK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "encoded_data = tokenizer.texts_to_sequences([text])[0]\n",
        "vocabulary_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "9l4PrRMaWT3O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the tokenizer object for future use\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "5nV3yMmTWga9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ICeaNh0COI-t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(encoded_data)):\n",
        "    sequence = encoded_data[:i+1]\n",
        "    sequences.append(sequence)\n",
        "\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=6, padding='pre'))\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocabulary_size)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fYv6Q98aWkUB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "zgTYOEpsYWls"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating word embeddings\n",
        "sentences = [sentence.split() for sentence in text.split('.')]\n",
        "model_gensim = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model_gensim.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "fK7ty_RnYL64"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gensim = Word2Vec.load(\"word2vec.model\")\n",
        "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = model_gensim.wv[word]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "\n",
        "        pass"
      ],
      "metadata": {
        "id": "RXUSiFhyYa03"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocabulary_size, 100, input_length=5),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True)),\n",
        "  tf.keras.layers.Dropout(0.3),\n",
        "  tf.keras.layers.LSTM(100),\n",
        "  tf.keras.layers.Dense(vocabulary_size, activation='softmax')\n",
        "])\n",
        "\n",
        "first_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oHeBauLEYsWz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the first model\n",
        "first_history = first_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIsqul7FOqbx",
        "outputId": "3ccc207c-a1e6-46f1-c678-27e079f56930"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "52/52 [==============================] - 6s 39ms/step - loss: 7.1397 - accuracy: 0.0236 - val_loss: 7.1428 - val_accuracy: 0.0314\n",
            "Epoch 2/100\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 6.8773 - accuracy: 0.0254 - val_loss: 7.4321 - val_accuracy: 0.0266\n",
            "Epoch 3/100\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 6.5728 - accuracy: 0.0254 - val_loss: 7.8139 - val_accuracy: 0.0266\n",
            "Epoch 4/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 6.3629 - accuracy: 0.0254 - val_loss: 7.9965 - val_accuracy: 0.0242\n",
            "Epoch 5/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 6.1825 - accuracy: 0.0254 - val_loss: 8.1026 - val_accuracy: 0.0290\n",
            "Epoch 6/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 6.0097 - accuracy: 0.0254 - val_loss: 8.5571 - val_accuracy: 0.0217\n",
            "Epoch 7/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 5.8236 - accuracy: 0.0248 - val_loss: 8.7730 - val_accuracy: 0.0217\n",
            "Epoch 8/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 5.6214 - accuracy: 0.0308 - val_loss: 9.0690 - val_accuracy: 0.0193\n",
            "Epoch 9/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 5.4320 - accuracy: 0.0332 - val_loss: 9.4400 - val_accuracy: 0.0193\n",
            "Epoch 10/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 5.2524 - accuracy: 0.0369 - val_loss: 9.5164 - val_accuracy: 0.0097\n",
            "Epoch 11/100\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 5.0540 - accuracy: 0.0447 - val_loss: 9.5916 - val_accuracy: 0.0121\n",
            "Epoch 12/100\n",
            "52/52 [==============================] - 2s 41ms/step - loss: 4.8641 - accuracy: 0.0532 - val_loss: 9.6112 - val_accuracy: 0.0097\n",
            "Epoch 13/100\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 4.6834 - accuracy: 0.0671 - val_loss: 9.9030 - val_accuracy: 0.0169\n",
            "Epoch 14/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.5039 - accuracy: 0.0785 - val_loss: 9.9816 - val_accuracy: 0.0072\n",
            "Epoch 15/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 4.3413 - accuracy: 0.0882 - val_loss: 10.1026 - val_accuracy: 0.0072\n",
            "Epoch 16/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 4.2353 - accuracy: 0.0991 - val_loss: 10.1834 - val_accuracy: 0.0097\n",
            "Epoch 17/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 4.1114 - accuracy: 0.1100 - val_loss: 10.2821 - val_accuracy: 0.0097\n",
            "Epoch 18/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.9585 - accuracy: 0.1245 - val_loss: 10.4334 - val_accuracy: 0.0072\n",
            "Epoch 19/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.8205 - accuracy: 0.1480 - val_loss: 10.5809 - val_accuracy: 0.0072\n",
            "Epoch 20/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.7170 - accuracy: 0.1631 - val_loss: 10.5836 - val_accuracy: 0.0097\n",
            "Epoch 21/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 3.5743 - accuracy: 0.1982 - val_loss: 10.8146 - val_accuracy: 0.0024\n",
            "Epoch 22/100\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 3.4541 - accuracy: 0.2296 - val_loss: 10.7590 - val_accuracy: 0.0048\n",
            "Epoch 23/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.3278 - accuracy: 0.2387 - val_loss: 10.8903 - val_accuracy: 0.0097\n",
            "Epoch 24/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.2139 - accuracy: 0.2870 - val_loss: 10.9271 - val_accuracy: 0.0072\n",
            "Epoch 25/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.0989 - accuracy: 0.3257 - val_loss: 11.0890 - val_accuracy: 0.0072\n",
            "Epoch 26/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.9988 - accuracy: 0.3378 - val_loss: 11.1573 - val_accuracy: 0.0072\n",
            "Epoch 27/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.8912 - accuracy: 0.3801 - val_loss: 11.2281 - val_accuracy: 0.0048\n",
            "Epoch 28/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.8004 - accuracy: 0.4085 - val_loss: 11.3407 - val_accuracy: 0.0072\n",
            "Epoch 29/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 2.7152 - accuracy: 0.4254 - val_loss: 11.4210 - val_accuracy: 0.0048\n",
            "Epoch 30/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.6018 - accuracy: 0.4695 - val_loss: 11.5170 - val_accuracy: 0.0048\n",
            "Epoch 31/100\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 2.4903 - accuracy: 0.5076 - val_loss: 11.4072 - val_accuracy: 0.0097\n",
            "Epoch 32/100\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 2.3992 - accuracy: 0.5378 - val_loss: 11.6661 - val_accuracy: 0.0048\n",
            "Epoch 33/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.3107 - accuracy: 0.5692 - val_loss: 11.7171 - val_accuracy: 0.0072\n",
            "Epoch 34/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.2175 - accuracy: 0.5927 - val_loss: 11.6967 - val_accuracy: 0.0072\n",
            "Epoch 35/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.1545 - accuracy: 0.6121 - val_loss: 11.8454 - val_accuracy: 0.0072\n",
            "Epoch 36/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.0889 - accuracy: 0.6109 - val_loss: 11.9822 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 2.0319 - accuracy: 0.6363 - val_loss: 11.9520 - val_accuracy: 0.0097\n",
            "Epoch 38/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.9319 - accuracy: 0.6640 - val_loss: 12.0438 - val_accuracy: 0.0072\n",
            "Epoch 39/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.8424 - accuracy: 0.6918 - val_loss: 12.0428 - val_accuracy: 0.0072\n",
            "Epoch 40/100\n",
            "52/52 [==============================] - 1s 25ms/step - loss: 1.7704 - accuracy: 0.7196 - val_loss: 12.1542 - val_accuracy: 0.0097\n",
            "Epoch 41/100\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 1.7112 - accuracy: 0.7239 - val_loss: 12.1210 - val_accuracy: 0.0048\n",
            "Epoch 42/100\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 1.6325 - accuracy: 0.7517 - val_loss: 12.2040 - val_accuracy: 0.0072\n",
            "Epoch 43/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5805 - accuracy: 0.7656 - val_loss: 12.2444 - val_accuracy: 0.0097\n",
            "Epoch 44/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.5025 - accuracy: 0.7825 - val_loss: 12.3383 - val_accuracy: 0.0097\n",
            "Epoch 45/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.4110 - accuracy: 0.8109 - val_loss: 12.3850 - val_accuracy: 0.0097\n",
            "Epoch 46/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.3424 - accuracy: 0.8308 - val_loss: 12.4331 - val_accuracy: 0.0097\n",
            "Epoch 47/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.2811 - accuracy: 0.8381 - val_loss: 12.4138 - val_accuracy: 0.0097\n",
            "Epoch 48/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 1.2315 - accuracy: 0.8508 - val_loss: 12.4654 - val_accuracy: 0.0097\n",
            "Epoch 49/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.1835 - accuracy: 0.8647 - val_loss: 12.5259 - val_accuracy: 0.0072\n",
            "Epoch 50/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 1.1373 - accuracy: 0.8713 - val_loss: 12.6177 - val_accuracy: 0.0048\n",
            "Epoch 51/100\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 1.0953 - accuracy: 0.8792 - val_loss: 12.6563 - val_accuracy: 0.0048\n",
            "Epoch 52/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0719 - accuracy: 0.8683 - val_loss: 12.6702 - val_accuracy: 0.0048\n",
            "Epoch 53/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 1.0078 - accuracy: 0.8937 - val_loss: 12.7606 - val_accuracy: 0.0072\n",
            "Epoch 54/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.9569 - accuracy: 0.8985 - val_loss: 12.7411 - val_accuracy: 0.0097\n",
            "Epoch 55/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.8934 - accuracy: 0.9190 - val_loss: 12.8042 - val_accuracy: 0.0072\n",
            "Epoch 56/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.8692 - accuracy: 0.9130 - val_loss: 12.8407 - val_accuracy: 0.0048\n",
            "Epoch 57/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.8195 - accuracy: 0.9202 - val_loss: 12.9862 - val_accuracy: 0.0072\n",
            "Epoch 58/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.7694 - accuracy: 0.9341 - val_loss: 12.9445 - val_accuracy: 0.0072\n",
            "Epoch 59/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.7237 - accuracy: 0.9360 - val_loss: 12.9801 - val_accuracy: 0.0072\n",
            "Epoch 60/100\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 0.7015 - accuracy: 0.9378 - val_loss: 13.0504 - val_accuracy: 0.0121\n",
            "Epoch 61/100\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 0.6590 - accuracy: 0.9414 - val_loss: 13.0543 - val_accuracy: 0.0048\n",
            "Epoch 62/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.6293 - accuracy: 0.9541 - val_loss: 13.1092 - val_accuracy: 0.0072\n",
            "Epoch 63/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.6185 - accuracy: 0.9468 - val_loss: 13.1498 - val_accuracy: 0.0048\n",
            "Epoch 64/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.5877 - accuracy: 0.9486 - val_loss: 13.1703 - val_accuracy: 0.0048\n",
            "Epoch 65/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.5628 - accuracy: 0.9583 - val_loss: 13.2052 - val_accuracy: 0.0072\n",
            "Epoch 66/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.5251 - accuracy: 0.9631 - val_loss: 13.2766 - val_accuracy: 0.0072\n",
            "Epoch 67/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.5164 - accuracy: 0.9613 - val_loss: 13.2682 - val_accuracy: 0.0048\n",
            "Epoch 68/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4884 - accuracy: 0.9656 - val_loss: 13.2922 - val_accuracy: 0.0072\n",
            "Epoch 69/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4611 - accuracy: 0.9728 - val_loss: 13.3046 - val_accuracy: 0.0048\n",
            "Epoch 70/100\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.4527 - accuracy: 0.9668 - val_loss: 13.3404 - val_accuracy: 0.0072\n",
            "Epoch 71/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 0.4334 - accuracy: 0.9698 - val_loss: 13.3790 - val_accuracy: 0.0048\n",
            "Epoch 72/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.4002 - accuracy: 0.9722 - val_loss: 13.4222 - val_accuracy: 0.0048\n",
            "Epoch 73/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.3786 - accuracy: 0.9776 - val_loss: 13.4484 - val_accuracy: 0.0048\n",
            "Epoch 74/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.3658 - accuracy: 0.9782 - val_loss: 13.5218 - val_accuracy: 0.0072\n",
            "Epoch 75/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.3524 - accuracy: 0.9825 - val_loss: 13.5566 - val_accuracy: 0.0048\n",
            "Epoch 76/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.3342 - accuracy: 0.9837 - val_loss: 13.5765 - val_accuracy: 0.0048\n",
            "Epoch 77/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.3138 - accuracy: 0.9825 - val_loss: 13.5503 - val_accuracy: 0.0097\n",
            "Epoch 78/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.3093 - accuracy: 0.9843 - val_loss: 13.6637 - val_accuracy: 0.0072\n",
            "Epoch 79/100\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 0.2903 - accuracy: 0.9873 - val_loss: 13.6415 - val_accuracy: 0.0097\n",
            "Epoch 80/100\n",
            "52/52 [==============================] - 2s 39ms/step - loss: 0.2683 - accuracy: 0.9879 - val_loss: 13.7022 - val_accuracy: 0.0097\n",
            "Epoch 81/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2624 - accuracy: 0.9879 - val_loss: 13.7118 - val_accuracy: 0.0048\n",
            "Epoch 82/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2632 - accuracy: 0.9897 - val_loss: 13.7027 - val_accuracy: 0.0072\n",
            "Epoch 83/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2601 - accuracy: 0.9903 - val_loss: 13.8003 - val_accuracy: 0.0072\n",
            "Epoch 84/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2645 - accuracy: 0.9831 - val_loss: 13.7130 - val_accuracy: 0.0072\n",
            "Epoch 85/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2484 - accuracy: 0.9867 - val_loss: 13.7702 - val_accuracy: 0.0072\n",
            "Epoch 86/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2320 - accuracy: 0.9885 - val_loss: 13.8177 - val_accuracy: 0.0097\n",
            "Epoch 87/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2212 - accuracy: 0.9903 - val_loss: 13.8396 - val_accuracy: 0.0072\n",
            "Epoch 88/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.2056 - accuracy: 0.9909 - val_loss: 13.9118 - val_accuracy: 0.0072\n",
            "Epoch 89/100\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 0.2157 - accuracy: 0.9885 - val_loss: 13.8563 - val_accuracy: 0.0072\n",
            "Epoch 90/100\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 0.1980 - accuracy: 0.9921 - val_loss: 13.9045 - val_accuracy: 0.0097\n",
            "Epoch 91/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 0.1868 - accuracy: 0.9921 - val_loss: 13.9425 - val_accuracy: 0.0072\n",
            "Epoch 92/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.1727 - accuracy: 0.9927 - val_loss: 13.9603 - val_accuracy: 0.0072\n",
            "Epoch 93/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.1729 - accuracy: 0.9946 - val_loss: 14.0048 - val_accuracy: 0.0072\n",
            "Epoch 94/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 0.1569 - accuracy: 0.9952 - val_loss: 13.9889 - val_accuracy: 0.0072\n",
            "Epoch 95/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.1494 - accuracy: 0.9952 - val_loss: 14.0837 - val_accuracy: 0.0097\n",
            "Epoch 96/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.1385 - accuracy: 0.9970 - val_loss: 14.0771 - val_accuracy: 0.0097\n",
            "Epoch 97/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 0.1412 - accuracy: 0.9958 - val_loss: 14.1262 - val_accuracy: 0.0072\n",
            "Epoch 98/100\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 0.1279 - accuracy: 0.9958 - val_loss: 14.1415 - val_accuracy: 0.0072\n",
            "Epoch 99/100\n",
            "52/52 [==============================] - 2s 40ms/step - loss: 0.1182 - accuracy: 0.9982 - val_loss: 14.1380 - val_accuracy: 0.0072\n",
            "Epoch 100/100\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 0.1120 - accuracy: 0.9976 - val_loss: 14.1943 - val_accuracy: 0.0097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocabulary_size, 100, weights=[embedding_matrix], input_length=5, trainable=False),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.LSTM(100),\n",
        "  tf.keras.layers.Dense(vocabulary_size, activation='softmax')\n",
        "])\n",
        "\n",
        "second_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "l8erw5UWZ4yG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the second model\n",
        "second_history = second_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ0h3_fpai-2",
        "outputId": "c38964d3-e6fb-447d-cff7-3e0d838b301d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "52/52 [==============================] - 6s 37ms/step - loss: 7.1347 - accuracy: 0.0205 - val_loss: 7.1536 - val_accuracy: 0.0314\n",
            "Epoch 2/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.9095 - accuracy: 0.0254 - val_loss: 7.3761 - val_accuracy: 0.0314\n",
            "Epoch 3/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 6.7512 - accuracy: 0.0254 - val_loss: 7.9761 - val_accuracy: 0.0314\n",
            "Epoch 4/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.7044 - accuracy: 0.0254 - val_loss: 8.2230 - val_accuracy: 0.0314\n",
            "Epoch 5/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.6889 - accuracy: 0.0254 - val_loss: 8.2178 - val_accuracy: 0.0314\n",
            "Epoch 6/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.6791 - accuracy: 0.0254 - val_loss: 8.2948 - val_accuracy: 0.0314\n",
            "Epoch 7/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.6730 - accuracy: 0.0254 - val_loss: 8.3920 - val_accuracy: 0.0314\n",
            "Epoch 8/100\n",
            "52/52 [==============================] - 2s 34ms/step - loss: 6.6639 - accuracy: 0.0254 - val_loss: 8.5853 - val_accuracy: 0.0314\n",
            "Epoch 9/100\n",
            "52/52 [==============================] - 2s 32ms/step - loss: 6.6569 - accuracy: 0.0254 - val_loss: 8.6031 - val_accuracy: 0.0314\n",
            "Epoch 10/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 6.6371 - accuracy: 0.0254 - val_loss: 8.4612 - val_accuracy: 0.0314\n",
            "Epoch 11/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.6214 - accuracy: 0.0254 - val_loss: 8.7219 - val_accuracy: 0.0314\n",
            "Epoch 12/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 6.6062 - accuracy: 0.0254 - val_loss: 8.7344 - val_accuracy: 0.0314\n",
            "Epoch 13/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.6013 - accuracy: 0.0254 - val_loss: 9.2073 - val_accuracy: 0.0314\n",
            "Epoch 14/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.5602 - accuracy: 0.0260 - val_loss: 9.2066 - val_accuracy: 0.0314\n",
            "Epoch 15/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.4706 - accuracy: 0.0266 - val_loss: 9.2515 - val_accuracy: 0.0290\n",
            "Epoch 16/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 6.2895 - accuracy: 0.0254 - val_loss: 9.5940 - val_accuracy: 0.0242\n",
            "Epoch 17/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 6.1250 - accuracy: 0.0248 - val_loss: 9.6962 - val_accuracy: 0.0266\n",
            "Epoch 18/100\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 5.9728 - accuracy: 0.0242 - val_loss: 9.9151 - val_accuracy: 0.0193\n",
            "Epoch 19/100\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 5.8428 - accuracy: 0.0272 - val_loss: 10.4073 - val_accuracy: 0.0242\n",
            "Epoch 20/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 5.7094 - accuracy: 0.0296 - val_loss: 10.3049 - val_accuracy: 0.0217\n",
            "Epoch 21/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 5.6024 - accuracy: 0.0236 - val_loss: 10.4687 - val_accuracy: 0.0242\n",
            "Epoch 22/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 5.4949 - accuracy: 0.0338 - val_loss: 10.9272 - val_accuracy: 0.0242\n",
            "Epoch 23/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 5.3843 - accuracy: 0.0326 - val_loss: 10.8801 - val_accuracy: 0.0266\n",
            "Epoch 24/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 5.2917 - accuracy: 0.0344 - val_loss: 10.9638 - val_accuracy: 0.0169\n",
            "Epoch 25/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 5.1772 - accuracy: 0.0363 - val_loss: 11.0044 - val_accuracy: 0.0266\n",
            "Epoch 26/100\n",
            "52/52 [==============================] - 1s 27ms/step - loss: 5.0826 - accuracy: 0.0350 - val_loss: 10.9541 - val_accuracy: 0.0266\n",
            "Epoch 27/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 4.9976 - accuracy: 0.0356 - val_loss: 11.0782 - val_accuracy: 0.0242\n",
            "Epoch 28/100\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 4.9267 - accuracy: 0.0411 - val_loss: 11.2226 - val_accuracy: 0.0193\n",
            "Epoch 29/100\n",
            "52/52 [==============================] - 2s 31ms/step - loss: 4.8439 - accuracy: 0.0441 - val_loss: 11.2934 - val_accuracy: 0.0242\n",
            "Epoch 30/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.7779 - accuracy: 0.0375 - val_loss: 11.2299 - val_accuracy: 0.0266\n",
            "Epoch 31/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.6894 - accuracy: 0.0423 - val_loss: 11.3936 - val_accuracy: 0.0266\n",
            "Epoch 32/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 4.6229 - accuracy: 0.0405 - val_loss: 11.3717 - val_accuracy: 0.0242\n",
            "Epoch 33/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.5749 - accuracy: 0.0483 - val_loss: 11.5266 - val_accuracy: 0.0242\n",
            "Epoch 34/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 4.5407 - accuracy: 0.0520 - val_loss: 11.5062 - val_accuracy: 0.0242\n",
            "Epoch 35/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.4682 - accuracy: 0.0544 - val_loss: 11.6547 - val_accuracy: 0.0242\n",
            "Epoch 36/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.3885 - accuracy: 0.0477 - val_loss: 11.7071 - val_accuracy: 0.0169\n",
            "Epoch 37/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 4.3492 - accuracy: 0.0580 - val_loss: 11.7474 - val_accuracy: 0.0242\n",
            "Epoch 38/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 4.2899 - accuracy: 0.0532 - val_loss: 11.7583 - val_accuracy: 0.0242\n",
            "Epoch 39/100\n",
            "52/52 [==============================] - 2s 37ms/step - loss: 4.2668 - accuracy: 0.0586 - val_loss: 11.9128 - val_accuracy: 0.0193\n",
            "Epoch 40/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.2199 - accuracy: 0.0616 - val_loss: 11.9458 - val_accuracy: 0.0217\n",
            "Epoch 41/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.1437 - accuracy: 0.0634 - val_loss: 11.9672 - val_accuracy: 0.0169\n",
            "Epoch 42/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 4.1024 - accuracy: 0.0610 - val_loss: 12.0089 - val_accuracy: 0.0193\n",
            "Epoch 43/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.0683 - accuracy: 0.0634 - val_loss: 12.0341 - val_accuracy: 0.0169\n",
            "Epoch 44/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 4.0514 - accuracy: 0.0653 - val_loss: 12.1156 - val_accuracy: 0.0169\n",
            "Epoch 45/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.9964 - accuracy: 0.0731 - val_loss: 12.2097 - val_accuracy: 0.0169\n",
            "Epoch 46/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.9352 - accuracy: 0.0779 - val_loss: 12.1655 - val_accuracy: 0.0145\n",
            "Epoch 47/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.9235 - accuracy: 0.0749 - val_loss: 12.2381 - val_accuracy: 0.0193\n",
            "Epoch 48/100\n",
            "52/52 [==============================] - 1s 24ms/step - loss: 3.8574 - accuracy: 0.0870 - val_loss: 12.3210 - val_accuracy: 0.0169\n",
            "Epoch 49/100\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 3.8375 - accuracy: 0.0834 - val_loss: 12.3396 - val_accuracy: 0.0169\n",
            "Epoch 50/100\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 3.7993 - accuracy: 0.0804 - val_loss: 12.4386 - val_accuracy: 0.0169\n",
            "Epoch 51/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.8058 - accuracy: 0.0822 - val_loss: 12.4798 - val_accuracy: 0.0169\n",
            "Epoch 52/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.7434 - accuracy: 0.0943 - val_loss: 12.4734 - val_accuracy: 0.0169\n",
            "Epoch 53/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.6906 - accuracy: 0.1069 - val_loss: 12.5712 - val_accuracy: 0.0169\n",
            "Epoch 54/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.6510 - accuracy: 0.1033 - val_loss: 12.5907 - val_accuracy: 0.0169\n",
            "Epoch 55/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.6143 - accuracy: 0.1027 - val_loss: 12.6351 - val_accuracy: 0.0145\n",
            "Epoch 56/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.5701 - accuracy: 0.1221 - val_loss: 12.6820 - val_accuracy: 0.0169\n",
            "Epoch 57/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.5841 - accuracy: 0.1069 - val_loss: 12.7318 - val_accuracy: 0.0145\n",
            "Epoch 58/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.5509 - accuracy: 0.1009 - val_loss: 12.7659 - val_accuracy: 0.0145\n",
            "Epoch 59/100\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 3.5269 - accuracy: 0.1227 - val_loss: 12.7786 - val_accuracy: 0.0169\n",
            "Epoch 60/100\n",
            "52/52 [==============================] - 2s 33ms/step - loss: 3.5176 - accuracy: 0.1148 - val_loss: 12.8484 - val_accuracy: 0.0145\n",
            "Epoch 61/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.4554 - accuracy: 0.1263 - val_loss: 12.8605 - val_accuracy: 0.0145\n",
            "Epoch 62/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.4119 - accuracy: 0.1335 - val_loss: 12.9098 - val_accuracy: 0.0145\n",
            "Epoch 63/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.3799 - accuracy: 0.1420 - val_loss: 13.0443 - val_accuracy: 0.0145\n",
            "Epoch 64/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.3884 - accuracy: 0.1353 - val_loss: 12.9850 - val_accuracy: 0.0169\n",
            "Epoch 65/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.3524 - accuracy: 0.1480 - val_loss: 13.0419 - val_accuracy: 0.0169\n",
            "Epoch 66/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.3221 - accuracy: 0.1414 - val_loss: 13.0786 - val_accuracy: 0.0169\n",
            "Epoch 67/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.3059 - accuracy: 0.1426 - val_loss: 13.1163 - val_accuracy: 0.0169\n",
            "Epoch 68/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.2581 - accuracy: 0.1644 - val_loss: 13.1723 - val_accuracy: 0.0121\n",
            "Epoch 69/100\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 3.2326 - accuracy: 0.1740 - val_loss: 13.1847 - val_accuracy: 0.0145\n",
            "Epoch 70/100\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 3.2449 - accuracy: 0.1541 - val_loss: 13.2365 - val_accuracy: 0.0121\n",
            "Epoch 71/100\n",
            "52/52 [==============================] - 1s 23ms/step - loss: 3.2632 - accuracy: 0.1577 - val_loss: 13.2305 - val_accuracy: 0.0145\n",
            "Epoch 72/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.1618 - accuracy: 0.1758 - val_loss: 13.2493 - val_accuracy: 0.0145\n",
            "Epoch 73/100\n",
            "52/52 [==============================] - 1s 26ms/step - loss: 3.1271 - accuracy: 0.1994 - val_loss: 13.4039 - val_accuracy: 0.0145\n",
            "Epoch 74/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 3.1111 - accuracy: 0.1927 - val_loss: 13.3505 - val_accuracy: 0.0145\n",
            "Epoch 75/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.1079 - accuracy: 0.1909 - val_loss: 13.3796 - val_accuracy: 0.0121\n",
            "Epoch 76/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.0925 - accuracy: 0.1885 - val_loss: 13.4663 - val_accuracy: 0.0121\n",
            "Epoch 77/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 3.0364 - accuracy: 0.2042 - val_loss: 13.4490 - val_accuracy: 0.0145\n",
            "Epoch 78/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 3.0140 - accuracy: 0.1994 - val_loss: 13.5696 - val_accuracy: 0.0145\n",
            "Epoch 79/100\n",
            "52/52 [==============================] - 2s 29ms/step - loss: 2.9887 - accuracy: 0.2193 - val_loss: 13.5544 - val_accuracy: 0.0145\n",
            "Epoch 80/100\n",
            "52/52 [==============================] - 2s 36ms/step - loss: 3.0074 - accuracy: 0.1940 - val_loss: 13.5535 - val_accuracy: 0.0145\n",
            "Epoch 81/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.9340 - accuracy: 0.2435 - val_loss: 13.6165 - val_accuracy: 0.0193\n",
            "Epoch 82/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.9145 - accuracy: 0.2465 - val_loss: 13.6984 - val_accuracy: 0.0145\n",
            "Epoch 83/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.8897 - accuracy: 0.2350 - val_loss: 13.6937 - val_accuracy: 0.0145\n",
            "Epoch 84/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.8768 - accuracy: 0.2453 - val_loss: 13.7384 - val_accuracy: 0.0121\n",
            "Epoch 85/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.8490 - accuracy: 0.2544 - val_loss: 13.7278 - val_accuracy: 0.0145\n",
            "Epoch 86/100\n",
            "52/52 [==============================] - 1s 22ms/step - loss: 2.8291 - accuracy: 0.2574 - val_loss: 13.7568 - val_accuracy: 0.0145\n",
            "Epoch 87/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.8240 - accuracy: 0.2665 - val_loss: 13.8198 - val_accuracy: 0.0121\n",
            "Epoch 88/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.8169 - accuracy: 0.2423 - val_loss: 13.7922 - val_accuracy: 0.0169\n",
            "Epoch 89/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.8087 - accuracy: 0.2634 - val_loss: 13.8464 - val_accuracy: 0.0193\n",
            "Epoch 90/100\n",
            "52/52 [==============================] - 2s 38ms/step - loss: 2.8371 - accuracy: 0.2508 - val_loss: 13.8703 - val_accuracy: 0.0121\n",
            "Epoch 91/100\n",
            "52/52 [==============================] - 1s 28ms/step - loss: 2.8339 - accuracy: 0.2399 - val_loss: 13.9327 - val_accuracy: 0.0169\n",
            "Epoch 92/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.7551 - accuracy: 0.2689 - val_loss: 13.9654 - val_accuracy: 0.0145\n",
            "Epoch 93/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.6965 - accuracy: 0.2888 - val_loss: 14.0054 - val_accuracy: 0.0145\n",
            "Epoch 94/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.6690 - accuracy: 0.2937 - val_loss: 14.0361 - val_accuracy: 0.0121\n",
            "Epoch 95/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.6391 - accuracy: 0.3166 - val_loss: 14.0581 - val_accuracy: 0.0121\n",
            "Epoch 96/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.6671 - accuracy: 0.3009 - val_loss: 14.0452 - val_accuracy: 0.0145\n",
            "Epoch 97/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.6216 - accuracy: 0.3076 - val_loss: 14.1012 - val_accuracy: 0.0193\n",
            "Epoch 98/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.5715 - accuracy: 0.3414 - val_loss: 14.1172 - val_accuracy: 0.0169\n",
            "Epoch 99/100\n",
            "52/52 [==============================] - 1s 21ms/step - loss: 2.5323 - accuracy: 0.3408 - val_loss: 14.1635 - val_accuracy: 0.0145\n",
            "Epoch 100/100\n",
            "52/52 [==============================] - 2s 30ms/step - loss: 2.5487 - accuracy: 0.3450 - val_loss: 14.1712 - val_accuracy: 0.0145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_first_model = first_history.history['val_loss'][-1]\n",
        "val_loss_second_model = second_history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Validation Loss - First Model: {val_loss_first_model}\")\n",
        "print(f\"Validation Loss - Second Model: {val_loss_second_model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffOqwHUTcadB",
        "outputId": "40b4b9a2-d52a-41df-e3da-ebf66ccc0550"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss - First Model: 14.19434928894043\n",
            "Validation Loss - Second Model: 14.171158790588379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "i8SGaY86RVpf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(history):\n",
        "    epoch_range = range (10)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Training and Validation loss')\n",
        "    training_loss = history.history.get('loss')\n",
        "    validation_loss = history.history.get('val_loss')\n",
        "    plt.plot(epoch_range, training_loss, label='Training loss')\n",
        "    plt.plot(epoch_range, validation_loss, label='Validation loss')\n",
        "    plt.legend(loc='lower left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Training and Validation accuracy')\n",
        "    training_accuracy = history.history.get('accuracy')\n",
        "    validation_accuracy = history.history.get('val_accuracy')\n",
        "    plt.plot(epoch_range, training_accuracy, label='Training accuracy')\n",
        "    plt.plot(epoch_range, validation_accuracy, label='Validation accuracy')\n",
        "    plt.legend(loc='lower left')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WwICBTahRLCp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose Second Model"
      ],
      "metadata": {
        "id": "WjcQzWRad6OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_model.save(\"second_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB7drpLCdw2x",
        "outputId": "f442275f-f429-4e74-d626-b757e122cf2f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gyWpFmo8R-rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the previously saved model\n",
        "model =  tf.keras.models.load_model('second_model.h5')\n",
        "\n",
        "def predict_next_words(model, tokenizer, text, num_words=1):\n",
        "    for _ in range(num_words):\n",
        "\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=5, padding='pre')\n",
        "\n",
        "\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "\n",
        "        text += \" \" + output_word\n",
        "\n",
        "    return ' '.join(text.split(' ')[-num_words:])\n",
        "\n",
        "\n",
        "# Ask the user for input\n",
        "user_input = input(\"Input 5 Shona Words: \")\n",
        "\n",
        "\n",
        "predicted_words = predict_next_words(model, tokenizer, user_input, num_words=3)\n",
        "print(f\"The next words might be: {predicted_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjyFk8wsc710",
        "outputId": "902ed203-25e9-4b4a-88d1-62a8c749e47a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 5 Shona Words: mamukasei baba naamai marara sei\n",
            "The next words might be: ndakagashirwa mhosva ruru\n"
          ]
        }
      ]
    }
  ]
}